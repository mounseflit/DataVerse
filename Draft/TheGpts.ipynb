{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f2bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Load Data\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        logging.info(f\"Successfully loaded {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Preprocess Data\n",
    "def preprocess_data(academic_df):\n",
    "    # Fill missing values with forward fill\n",
    "    academic_df.ffill(inplace=True)\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    categorical_features = ['SchoolDepartment', 'CourseTitle', 'RequiredSkill', 'GainedSkill']\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    encoded_features = encoder.fit_transform(academic_df[categorical_features])\n",
    "\n",
    "    # Handle 'Hour' column if it exists\n",
    "    if 'Hour' in academic_df.columns:\n",
    "        try:\n",
    "            academic_df['Hour'] = pd.to_datetime(academic_df['Hour'], format='%H:%M:%S', errors='coerce').dt.hour\n",
    "            academic_df['Hour'] = academic_df['Hour'].fillna(0).astype(float)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to parse 'Hour' column: {e}\")\n",
    "            academic_df['Hour'] = 0\n",
    "    else:\n",
    "        logging.warning(\"Column 'Hour' does not exist.\")\n",
    "        academic_df['Hour'] = 0\n",
    "\n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numeric_features = scaler.fit_transform(academic_df[['Hour']])\n",
    "\n",
    "    # Combine encoded and numerical features\n",
    "    features = np.hstack((encoded_features, numeric_features))\n",
    "    return features, encoder, scaler\n",
    "\n",
    "\n",
    "# PCA for Dimensionality Reduction\n",
    "def preprocess_with_pca(features, n_components=2):\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    reduced_features = pca.fit_transform(features)\n",
    "    logging.info(f\"PCA reduced features to {n_components} dimensions.\")\n",
    "    return reduced_features\n",
    "\n",
    "\n",
    "# Optimize KMeans\n",
    "def optimize_kmeans(features):\n",
    "    best_k = 0\n",
    "    best_score = -1\n",
    "    for k in range(2, 11):  # Try cluster sizes from 2 to 10\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        clusters = kmeans.fit_predict(features)\n",
    "        score = silhouette_score(features, clusters)\n",
    "        if score > best_score:\n",
    "            best_k = k\n",
    "            best_score = score\n",
    "    logging.info(f\"Optimal number of clusters: {best_k} with Silhouette Score: {best_score}\")\n",
    "    return best_k\n",
    "\n",
    "\n",
    "# Train DBSCAN with Parameter Tuning\n",
    "def train_dbscan(features):\n",
    "    best_eps = 0.1\n",
    "    best_score = -1\n",
    "    best_model = None\n",
    "    for eps in np.arange(0.1, 1.0, 0.1):  # Test a range of `eps` values\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "        clusters = dbscan.fit_predict(features)\n",
    "        if len(set(clusters)) > 1:  # Ensure at least two clusters\n",
    "            score = silhouette_score(features, clusters)\n",
    "            if score > best_score:\n",
    "                best_eps = eps\n",
    "                best_score = score\n",
    "                best_model = dbscan\n",
    "    if best_model is None:\n",
    "        logging.warning(\"DBSCAN did not produce valid clusters.\")\n",
    "        return None, None\n",
    "    logging.info(f\"DBSCAN optimal eps: {best_eps} with Silhouette Score: {best_score}\")\n",
    "    return best_model, best_model.fit_predict(features)\n",
    "\n",
    "\n",
    "# Evaluate Models\n",
    "def evaluate_models(features, models):\n",
    "    scores = {}\n",
    "    for name, (model, clusters) in models.items():\n",
    "        if clusters is None or len(set(clusters)) <= 1:\n",
    "            logging.warning(f\"{name} did not create valid clusters.\")\n",
    "            continue\n",
    "        sil_score = silhouette_score(features, clusters)\n",
    "        calinski_harabasz = calinski_harabasz_score(features, clusters)\n",
    "        davies_bouldin = davies_bouldin_score(features, clusters)\n",
    "        scores[name] = {\n",
    "            'Silhouette Score': sil_score,\n",
    "            'Calinski-Harabasz Index': calinski_harabasz,\n",
    "            'Davies-Bouldin Index': davies_bouldin\n",
    "        }\n",
    "        logging.info(f\"{name} - Silhouette Score: {sil_score}\")\n",
    "        logging.info(f\"{name} - Calinski-Harabasz Index: {calinski_harabasz}\")\n",
    "        logging.info(f\"{name} - Davies-Bouldin Index: {davies_bouldin}\")\n",
    "    return scores\n",
    "\n",
    "\n",
    "# Save Models\n",
    "def save_models(models, encoder, scaler):\n",
    "    try:\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        for name, (model, _) in models.items():\n",
    "            if model is not None:\n",
    "                with open(f\"models/{name}_model.pkl\", 'wb') as file:\n",
    "                    pickle.dump(model, file)\n",
    "        with open(\"models/encoder.pkl\", 'wb') as file:\n",
    "            pickle.dump(encoder, file)\n",
    "        with open(\"models/scaler.pkl\", 'wb') as file:\n",
    "            pickle.dump(scaler, file)\n",
    "        logging.info(\"Models and encoders saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving models and encoders: {e}\")\n",
    "\n",
    "\n",
    "# Main Function\n",
    "def main():\n",
    "    # Load datasets\n",
    "    academic_df = load_data(os.path.join('data', 'academic.csv'))\n",
    "    department_df = load_data(os.path.join('data', 'department.csv'))\n",
    "\n",
    "    if academic_df.empty or department_df.empty:\n",
    "        logging.error(\"One or more datasets could not be loaded. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Preprocessing\n",
    "    features, encoder, scaler = preprocess_data(academic_df)\n",
    "\n",
    "    # Apply PCA\n",
    "    reduced_features = preprocess_with_pca(features, n_components=2)\n",
    "\n",
    "    # Optimize KMeans\n",
    "    optimal_k = optimize_kmeans(reduced_features)\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    kmeans_clusters = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "    # Train DBSCAN with optimal parameters\n",
    "    dbscan_model, dbscan_clusters = train_dbscan(reduced_features)\n",
    "\n",
    "    # Evaluate models\n",
    "    models = {\n",
    "        \"KMeans\": (kmeans, kmeans_clusters),\n",
    "        \"DBSCAN\": (dbscan_model, dbscan_clusters)\n",
    "    }\n",
    "    scores = evaluate_models(reduced_features, models)\n",
    "\n",
    "    # Save models and encoders\n",
    "    save_models(models, encoder, scaler)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
